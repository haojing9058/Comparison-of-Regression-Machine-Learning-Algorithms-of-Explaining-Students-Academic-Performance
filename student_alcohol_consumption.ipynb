{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data into Mdf and Pdf\n",
    "Mdf = pd.read_csv('student-mat.csv',sep=';')\n",
    "Pdf = pd.read_csv('student-por.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add a column \"Subject\" that describes which course the student has taken\n",
    "Mdf['Subject'] = 'M'\n",
    "Pdf['Subject'] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Identify student who took both Math and Portuguese classes\n",
    "df = pd.merge(Mdf,Pdf,on=[\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"], suffixes=('_M','_P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jing/anaconda/lib/python3.6/site-packages/pandas/core/generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dalc           int64\n",
       "Fedu           int64\n",
       "Fjob          object\n",
       "G1             int64\n",
       "G2             int64\n",
       "Medu           int64\n",
       "Mjob          object\n",
       "Pstatus       object\n",
       "Subject       object\n",
       "Walc           int64\n",
       "absences       int64\n",
       "activities    object\n",
       "address       object\n",
       "age            int64\n",
       "failures       int64\n",
       "famrel         int64\n",
       "famsize       object\n",
       "famsup        object\n",
       "freetime       int64\n",
       "goout          int64\n",
       "guardian      object\n",
       "health         int64\n",
       "higher        object\n",
       "internet      object\n",
       "G3             int64\n",
       "nursery       object\n",
       "paid          object\n",
       "reason        object\n",
       "romantic      object\n",
       "school        object\n",
       "schoolsup     object\n",
       "sex           object\n",
       "studytime      int64\n",
       "traveltime     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In df, replace G3 with an average score of Math and Portuguese. \n",
    "df.loc[:,'new_G3'] = (df.G3_M + df.G3_P)/2\n",
    "#Merge df back with Mdf, so we have a dataset (Mdf4) for students who take math only and who take both classes with G3 representing the average score.\n",
    "Mdf2 = df.drop(['guardian_P', 'traveltime_P',\n",
    "       'studytime_P', 'failures_P', 'schoolsup_P', 'famsup_P', 'paid_P',\n",
    "       'activities_P', 'higher_P', 'romantic_P', 'famrel_P', 'freetime_P',\n",
    "       'goout_P', 'Dalc_P', 'Walc_P', 'health_P', 'absences_P', 'G1_P', 'G2_P',\n",
    "       'G3_P', 'Subject_P'], axis=1)\n",
    "Mdf2.columns = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities',\n",
    "       'nursery', 'higher', 'internet', 'romantic', 'famrel',\n",
    "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences',\n",
    "       'G1', 'G2', 'G3', 'Subject', 'new_G3']\n",
    "Mdf3 = pd.concat([Mdf, Mdf2])\n",
    "Mdf3[['G3','new_G3']]\n",
    "Mdf4 = Mdf3.drop_duplicates(['Dalc', 'Fedu', 'Fjob', 'G1', 'G2', 'G3', 'Medu', 'Mjob', 'Pstatus',\n",
    "       'Subject', 'Walc', 'absences', 'activities', 'address', 'age',\n",
    "       'failures', 'famrel', 'famsize', 'famsup', 'freetime', 'goout',\n",
    "       'guardian', 'health', 'higher', 'internet', 'nursery', 'paid',\n",
    "       'reason', 'romantic', 'school', 'schoolsup', 'sex', 'studytime',\n",
    "       'traveltime'], keep='last')\n",
    "Mdf4.Subject = np.where(Mdf4.new_G3.isnull(), Mdf4.Subject, 'B')\n",
    "Mdf4.new_G3 = np.where(Mdf4.new_G3.isnull(), Mdf4.G3, Mdf4.new_G3)\n",
    "Mdf4 = Mdf4.drop('G3', axis=1)\n",
    "Mdf4 = Mdf4.rename(columns = {'new_G3':'G3'})\n",
    "Mdf4.G3 = Mdf4.G3.astype(int)\n",
    "Mdf4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dalc           int64\n",
       "Fedu           int64\n",
       "Fjob          object\n",
       "G1             int64\n",
       "G2             int64\n",
       "G3             int64\n",
       "Medu           int64\n",
       "Mjob          object\n",
       "Pstatus       object\n",
       "Subject       object\n",
       "Walc           int64\n",
       "absences       int64\n",
       "activities    object\n",
       "address       object\n",
       "age            int64\n",
       "failures       int64\n",
       "famrel         int64\n",
       "famsize       object\n",
       "famsup        object\n",
       "freetime       int64\n",
       "goout          int64\n",
       "guardian      object\n",
       "health         int64\n",
       "higher        object\n",
       "internet      object\n",
       "nursery       object\n",
       "paid          object\n",
       "reason        object\n",
       "romantic      object\n",
       "school        object\n",
       "schoolsup     object\n",
       "sex           object\n",
       "studytime      int64\n",
       "traveltime     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify students who took Portuguese class only (Pdf4).\n",
    "Pdf2 = df.drop(['guardian_M', 'traveltime_M', 'studytime_M',\n",
    "       'failures_M', 'schoolsup_M', 'famsup_M', 'paid_M', 'activities_M',\n",
    "       'higher_M', 'romantic_M', 'famrel_M',\n",
    "       'freetime_M', 'goout_M', 'Dalc_M', 'Walc_M', 'health_M', 'absences_M',\n",
    "       'G1_M', 'G2_M', 'G3_M', 'Subject_M'], axis=1)\n",
    "Pdf2.columns = (['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'nursery', 'internet', 'guardian',\n",
    "       'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup',\n",
    "       'paid', 'activities', 'higher', 'romantic', 'famrel',\n",
    "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences',\n",
    "       'G1', 'G2', 'G3', 'Subject', 'new_G3'])\n",
    "Pdf3 = pd.concat([Pdf, Pdf2])\n",
    "Pdf4 = Pdf3.drop_duplicates([\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"], keep=False)\n",
    "Pdf4 = Pdf4.drop('new_G3', axis=1)\n",
    "Pdf4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>G3</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Walc</th>\n",
       "      <th>absences</th>\n",
       "      <th>...</th>\n",
       "      <th>internet</th>\n",
       "      <th>nursery</th>\n",
       "      <th>reason</th>\n",
       "      <th>romantic</th>\n",
       "      <th>school</th>\n",
       "      <th>schoolsup</th>\n",
       "      <th>sex</th>\n",
       "      <th>studytime</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>courses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>reputation</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>course</td>\n",
       "      <td>yes</td>\n",
       "      <td>GP</td>\n",
       "      <td>yes</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>course</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>course</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>services</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>T</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>course</td>\n",
       "      <td>no</td>\n",
       "      <td>GP</td>\n",
       "      <td>no</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dalc  Fedu      Fjob  G3  Medu      Mjob Pstatus Subject  Walc  absences  \\\n",
       "128     1     2     other   0     2  services       T       M     2         0   \n",
       "161     1     2     other   7     3     other       T       M     4         6   \n",
       "162     2     2     other   0     1     other       T       M     4         0   \n",
       "163     1     3  services  10     1   at_home       T       M     4         2   \n",
       "165     1     2  services  12     3  services       T       M     1        16   \n",
       "\n",
       "      ...   internet nursery      reason  romantic  school schoolsup sex  \\\n",
       "128   ...        yes     yes  reputation        no      GP        no   M   \n",
       "161   ...        yes     yes      course       yes      GP       yes   M   \n",
       "162   ...         no     yes      course        no      GP        no   M   \n",
       "163   ...        yes     yes      course        no      GP        no   M   \n",
       "165   ...         no      no      course        no      GP        no   M   \n",
       "\n",
       "     studytime  traveltime courses  \n",
       "128          1           1       1  \n",
       "161          2           2       1  \n",
       "162          1           2       1  \n",
       "163          1           1       1  \n",
       "165          1           2       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate Mdf4 and Pdft. \n",
    "INPUTdf = pd.concat([Mdf4,Pdf4])\n",
    "INPUTdf = INPUTdf.drop(['G1', 'G2', 'paid'], axis=1) #paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "#Add a column \"courses\" that has the number of course the student has taken\n",
    "INPUTdf['courses'] = np.where(INPUTdf['Subject'] == 'B', 2, 1)\n",
    "#The new dataset INPUTdf representing students who have taken Math, Portuguese, or both classes\n",
    "INPUTdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dummy code categorical variables\n",
    "INPUTdf = pd.get_dummies(INPUTdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dalc', 'Fedu', 'G3', 'Medu', 'Walc', 'absences', 'age', 'failures',\n",
       "       'famrel', 'freetime', 'goout', 'health', 'studytime', 'traveltime',\n",
       "       'courses', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services',\n",
       "       'Fjob_teacher', 'Mjob_at_home', 'Mjob_health', 'Mjob_other',\n",
       "       'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Subject_B',\n",
       "       'Subject_M', 'Subject_P', 'activities_no', 'activities_yes',\n",
       "       'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'famsup_no',\n",
       "       'famsup_yes', 'guardian_father', 'guardian_mother', 'guardian_other',\n",
       "       'higher_no', 'higher_yes', 'internet_no', 'internet_yes', 'nursery_no',\n",
       "       'nursery_yes', 'reason_course', 'reason_home', 'reason_other',\n",
       "       'reason_reputation', 'romantic_no', 'romantic_yes', 'school_GP',\n",
       "       'school_MS', 'schoolsup_no', 'schoolsup_yes', 'sex_F', 'sex_M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUTdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split rawData into trainData and testData\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = INPUTdf['G3']\n",
    "X = INPUTdf[['Dalc', 'Fedu', 'Medu', 'Walc', 'absences', 'age', 'failures',\n",
    "       'famrel', 'freetime', 'goout', 'health', 'studytime', 'traveltime',\n",
    "       'courses', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services',\n",
    "       'Fjob_teacher', 'Mjob_at_home', 'Mjob_health', 'Mjob_other',\n",
    "       'Mjob_services', 'Mjob_teacher', 'Pstatus_A', 'Pstatus_T', 'Subject_B',\n",
    "       'Subject_M', 'Subject_P', 'activities_no', 'activities_yes',\n",
    "       'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'famsup_no',\n",
    "       'famsup_yes', 'guardian_father', 'guardian_mother', 'guardian_other',\n",
    "       'higher_no', 'higher_yes', 'internet_no', 'internet_yes', 'nursery_no',\n",
    "       'nursery_yes', 'reason_course', 'reason_home', 'reason_other',\n",
    "       'reason_reputation', 'romantic_no', 'romantic_yes', 'school_GP',\n",
    "       'school_MS', 'schoolsup_no', 'schoolsup_yes', 'sex_F', 'sex_M']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Seperate X_train and X_test into sub dataframes consists of dummy columns only and non-dummy columns only\n",
    "X_train_nondummies = X_train.select_dtypes(exclude=['uint8'])\n",
    "X_train_dummies = X_train.select_dtypes(exclude=['int64'])\n",
    "X_test_nondummies = X_test.select_dtypes(exclude=['uint8'])\n",
    "X_test_dummies = X_test.select_dtypes(exclude=['int64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Center and scale non-dummy coded varialbes in training dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_nondummies)\n",
    "X_train_nondummies_scaled = scaler.transform(X_train_nondummies)\n",
    "X_train_nondummies_scaled.shape\n",
    "#This obtaines a 464 by 14 numpy array as non-dummy coded training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 44)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert dataframe X_train_dummies into numpy array X_train_dummies_np\n",
    "X_train_dummies_np = X_train_dummies.values\n",
    "X_train_dummies_np.shape\n",
    "#This obtaines a 464 by 44 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 58)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate scaled-nondummy and dummy variables as new training data X\n",
    "X_train_new = np.concatenate((X_train_nondummies_scaled, X_train_dummies_np), axis=1)\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53554777,  0.63014511,  1.34476206, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.53554777,  1.53257513,  1.34476206, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.53554777, -1.17471495, -1.32939335, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 2.85300904,  0.63014511,  0.45337692, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.7234901 , -1.17471495, -1.32939335, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-0.53554777,  0.63014511,  1.34476206, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using similar approach to process X_test datasets\n",
    "#Transform test dataset\n",
    "X_test_nondummies_scaled = scaler.transform(X_test_nondummies)\n",
    "#Convert dataframe X_test_dummies into numpy array X_test_dummies_np\n",
    "X_test_dummies_np = X_test_dummies.values\n",
    "#Concatenate scaled-nondummy and dummy variables as new test data X\n",
    "X_test_new = np.concatenate((X_test_nondummies_scaled, X_test_dummies_np), axis=1)\n",
    "X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.049787068367863944, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma=0.049787068367863944, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "model = svm.SVR(kernel='rbf', C=np.exp(-3), gamma=np.exp(-3))\n",
    "model.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042114700113775938"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_rbf = SVR(kernel='rbf', C=np.exp(-3), gamma=0.1)\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.063878056212857914"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(-3), gamma=np.exp(-3))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25860216762260924"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(-1), gamma=np.exp(-3))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64850968470407322"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(1), gamma=np.exp(-3))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99919908213388131"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(10), gamma=np.exp(10))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73095821928214399"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(1), gamma=np.exp(10))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99910752006031334"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(10), gamma=np.exp(-3))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73095821928214399"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(1), gamma=np.exp(100))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99910752006031334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(50), gamma=np.exp(-3))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99920886579642787"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(10), gamma=np.exp(-1))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99919908213388131"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=np.exp(10), gamma=np.exp(50))\n",
    "y_rbf = svr_rbf.fit(X_train_new, y_train)\n",
    "svr_rbf.score(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
